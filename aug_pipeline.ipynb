{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from imgaug import augmenters as ia\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc; gc.enable()\n",
    "\n",
    "# here = os.path.dirname(os.path.abspath(__file__))\n",
    "PATH_IN =  '/Users/flatironschool/Documents/Kaggle/Kaggle-DR-detection/'\n",
    "PATH_IMG = PATH_IN + \"dataset/train_images/\"\n",
    "PATH_OUT = PATH_IN + 'output/augmented/'\n",
    "# body\n",
    "\n",
    "# read in the list of training images\n",
    "df = pd.read_csv(PATH_IN + \"train.csv\")\n",
    "# df = df.head().copy()\n",
    "gc.collect()\n",
    "\n",
    "# idiot-test with first image\n",
    "id_code = df['id_code'].values[0]\n",
    "label = df['diagnosis'].values[0]\n",
    "labels = {}\n",
    "#\n",
    "# SKIP\n",
    "# do the thing... (loop)\n",
    "flip_h = ia.Fliplr(1)\n",
    "flip_v = ia.Flipud(1)\n",
    "g_blur = ia.GaussianBlur(sigma=(0.0, 5.0))\n",
    "rotate = ia.Affine(rotate=(-180, 180))\n",
    "shear = ia.Affine(shear=(-5, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(id_code):\n",
    "    img_path = PATH_IMG+id_code+'.png'\n",
    "    img = cv2.imread(img_path)\n",
    "#     plt.imshow(img)\n",
    "    return img\n",
    "\n",
    "def getRadius(img):\n",
    "    circles = cv2.HoughCircles(img,3,1,max(img.shape)/2,param1=50,param2=30,minRadius= int(max(img.shape)/6),maxRadius=max(img.shape))\n",
    "    return circles[0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df.index:\n",
    "    id_code = df['id_code'][ind]\n",
    "    diag = df['diagnosis'][ind]\n",
    "    img_array = get_img(id_code)\n",
    "    #crop each image to 80% of fundus diameter\n",
    "    try:\n",
    "        r = getRadius(img_array)\n",
    "        crop_r = round(r*0.8) \n",
    "    except:\n",
    "        crop_r = (img_array.shape[1]/2)*0.8\n",
    "            \n",
    "    center_x = int(img_array.shape[1]/2)\n",
    "    center_y = int(img_array.shape[0]/2)\n",
    "    \n",
    "    left_border = int(center_x - crop_r)\n",
    "    right_border = int(center_x + crop_r)\n",
    "    upper_border = int(center_y - crop_r)\n",
    "    lower_border = int(center_y + crop_r)\n",
    "    \n",
    "    if left_border < 0:\n",
    "        difference = 0- left_border\n",
    "        left_border = 0\n",
    "        right_border -=difference\n",
    "        upper_border += difference\n",
    "        lower_border -= difference\n",
    "    if upper_border < 0:\n",
    "        difference = 0- upper_border\n",
    "        upper_border = 0\n",
    "        lower_border -= difference\n",
    "        left_border += difference\n",
    "        right_border -= difference\n",
    "\n",
    "    cropped = img_array[upper_border:lower_border, left_border:right_border]\n",
    "    #resize image to resnet's expected input size\n",
    "    img = cv2.resize(cropped, (224,224), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    h = id_code +'_h'\n",
    "    v = id_code + '_v'\n",
    "    rot = id_code +'_rot'\n",
    "    sh = id_code + '_sh'\n",
    "    blur = id_code + '_blur'\n",
    "    cv2.imwrite(PATH_OUT + h + \".png\",flip_h.augment_image(img))\n",
    "    cv2.imwrite(PATH_OUT + v + \".png\",flip_v.augment_image(img))\n",
    "    cv2.imwrite(PATH_OUT + rot+ '.png',rotate.augment_image(img))\n",
    "    cv2.imwrite(PATH_OUT + sh + \".png\",shear.augment_image(img))\n",
    "    cv2.imwrite(PATH_OUT + blur + '.png',g_blur.augment_image(img)) \n",
    "    labels.update(dict.fromkeys([h,v,rot,sh,blur],diag))\n",
    "    # for each augmentation\n",
    "        # save new image\n",
    "        # update new list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = pd.DataFrame()\n",
    "lbls = lbls.from_dict(labels, orient='index', columns = ['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls.to_csv(PATH_IN+'drlabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/flatironschool/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/.local/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d663e490caf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for file in output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1096\u001b[0;31m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    351\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (224, 224, 3)"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for ind in lbls.index:\n",
    "    img_array = cv2.imread(PATH_OUT+ind+'.png')\n",
    "#for file in output\n",
    "    print(img_array.shape)\n",
    "    feature = model.predict(img_array)\n",
    "    flat = feature.flatten()\n",
    "    features.append(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f_data = h5py.File(features_path, 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(features))\n",
    "h5f_label = h5py.File(labels_path, 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(le_labels))\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "# save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(model_path + str(test_size) + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# save weights\n",
    "model.save_weights(model_path + str(test_size) + \".h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
